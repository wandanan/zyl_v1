# story_retrieval_test.py

import torch
import numpy as np
from typing import List, Tuple, Dict, Any
import time
import json
import logging

# ã€ç¡®è®¤å¯¼å…¥V5ã€‘
from advanced_zipper_engine_v5 import AdvancedZipperQueryEngine, AdvancedZipperConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, # INFOåªçœ‹å…³é”®æ­¥éª¤, DEBUGå¯çœ‹æƒé‡é€‰æ‹©ç­‰ç»†èŠ‚
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('story_retrieval_test.log', mode='w', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# æ£€æŸ¥CUDAå¯ç”¨æ€§
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    logger.info(f"GPUå‹å·: {torch.cuda.get_device_name()}")

def load_story_dataset(json_file_path: str) -> Dict[str, Any]:
    """åŠ è½½æ•…äº‹æ•°æ®é›†"""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        logger.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†: {json_file_path}")
        logger.info(f"åŒ…å« {len(dataset['stories'])} ä¸ªæ•…äº‹")
        return dataset
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        raise

def calculate_mrr_and_hit_rate(expected: List[int], retrieved: List[int], k: int) -> Tuple[float, float]:
    """è®¡ç®—MRR@kå’ŒHitRate@k"""
    if not expected or not retrieved:
        return 0.0, 0.0
    
    mrr_score, hit = 0.0, 0.0
    retrieved_at_k = retrieved[:k]
    
    for i, doc_id in enumerate(retrieved_at_k):
        if doc_id in expected:
            mrr_score = 1.0 / (i + 1)
            break
            
    if len(set(expected) & set(retrieved_at_k)) > 0:
        hit = 1.0
        
    return mrr_score, hit


def evaluate_retrieval_performance(engine: AdvancedZipperQueryEngine, dataset: Dict[str, Any], k_val: int) -> Dict[str, Any]:
    """è¯„ä¼°æ£€ç´¢æ€§èƒ½"""
    logger.info("å¼€å§‹è¯„ä¼°æ£€ç´¢æ€§èƒ½...")
    all_results = []
    
    for story in dataset['stories']:
        story_id, story_title, story_content = story['story_id'], story['title'], story['content']
        logger.info(f"\n--- å¤„ç†æ•…äº‹: {story_title} ({story_id}) ---")
        engine.build_document_index(story_content)
        
        for question_data in story['questions']:
            question, expected_fragments = question_data['question'], question_data['expected_fragments']
            logger.info(f"  -> æµ‹è¯•é—®é¢˜: {question}")
            
            start_time = time.time()
            results = engine.zipper_retrieve(question, top_k=k_val)
            retrieval_time = time.time() - start_time
            
            retrieved_fragments = [doc_id for doc_id, score, doc in results]
            
            mrr_score, hit_rate = calculate_mrr_and_hit_rate(expected_fragments, retrieved_fragments, k=k_val)
            
            result = {
                'story_id': story_id, 'story_title': story_title, 'question': question,
                'expected_fragments': expected_fragments, 'retrieved_fragments': retrieved_fragments,
                'mrr_score': mrr_score, 'hit_rate': hit_rate, 'retrieval_time': retrieval_time,
                'difficulty': question_data['difficulty'], 'type': question_data['type'],
            }
            
            all_results.append(result)
            
            logger.info(f"     æœŸæœ›ç‰‡æ®µ: {expected_fragments}")
            logger.info(f"     æ£€ç´¢ç‰‡æ®µ: {retrieved_fragments}")
            logger.info(f"     Hit@{k_val}: {'âœ…' if hit_rate > 0 else 'âŒ'}")
            logger.info(f"     MRR@{k_val}: {mrr_score:.3f}")
            logger.info(f"     è€—æ—¶: {retrieval_time:.3f}s")
            
    return {'all_results': all_results}

def analyze_and_print_results(evaluation_results: Dict[str, Any], k_val: int):
    """åˆ†æå¹¶æ‰“å°æœ€ç»ˆçš„ç»Ÿè®¡ç»“æœ"""
    all_results = evaluation_results['all_results']
    if not all_results:
        logger.warning("æ²¡æœ‰å¯ä¾›åˆ†æçš„ç»“æœã€‚")
        return

    total_questions = len(all_results)
    avg_mrr = sum(r['mrr_score'] for r in all_results) / total_questions
    avg_hit_rate = sum(r['hit_rate'] for r in all_results) / total_questions
    avg_time = sum(r['retrieval_time'] for r in all_results) / total_questions

    type_stats, difficulty_stats = {}, {}
    for r in all_results:
        q_type, q_diff = r['type'], r['difficulty']
        if q_type not in type_stats: type_stats[q_type] = []
        if q_diff not in difficulty_stats: difficulty_stats[q_diff] = []
        type_stats[q_type].append(r)
        difficulty_stats[q_diff].append(r)

    logger.info("\n" + "="*80)
    logger.info(f"=== æ£€ç´¢æµ‹è¯•ç»“æœæ€»ç»“ (k={k_val}) ===")
    logger.info("="*80)
    
    logger.info(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:\n   - æ€»é—®é¢˜æ•°: {total_questions}\n   - å¹³å‡å€’æ•°æ’å (MRR@{k_val}): {avg_mrr:.3f}\n   - æ€»ä½“å‘½ä¸­ç‡ (HitRate@{k_val}): {avg_hit_rate:.1%}\n   - å¹³å‡æ£€ç´¢æ—¶é—´: {avg_time:.3f} ç§’/æŸ¥è¯¢")
    
    logger.info("\nğŸ“ æŒ‰é—®é¢˜ç±»å‹åˆ†æ:")
    for q_type, results in sorted(type_stats.items()):
        count = len(results)
        mrr = sum(r['mrr_score'] for r in results) / count if count > 0 else 0
        hit = sum(r['hit_rate'] for r in results) / count if count > 0 else 0
        logger.info(f"   - {q_type.capitalize():<10} ({count:>2}ä¸ª): MRR@{k_val}={mrr:.3f}, HitRate@{k_val}={hit:.1%}")

    logger.info("\nğŸ“ˆ æŒ‰éš¾åº¦åˆ†æ:")
    for q_diff, results in sorted(difficulty_stats.items()):
        count = len(results)
        mrr = sum(r['mrr_score'] for r in results) / count if count > 0 else 0
        hit = sum(r['hit_rate'] for r in results) / count if count > 0 else 0
        logger.info(f"   - {q_diff.capitalize():<10} ({count:>2}ä¸ª): MRR@{k_val}={mrr:.3f}, HitRate@{k_val}={hit:.1%}")
    
    logger.info("="*80)

def save_results(evaluation_results: Dict[str, Any], output_file: str = "retrieval_results.json"):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    logger.info(f"\næ­£åœ¨ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœåˆ°: {output_file}")
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»“æœå·²æˆåŠŸä¿å­˜ã€‚")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}", exc_info=True)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("="*80 + "\n=== V5 åŠ¨æ€è‡ªé€‚åº”å¼•æ“æµ‹è¯•å¼€å§‹ ===\n" + "="*80)
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_story_dataset('story_qa_dataset.json')
    
    # åˆ›å»ºV5é…ç½®
    config = AdvancedZipperConfig()
    
    # åˆå§‹åŒ–å¼•æ“
    engine = AdvancedZipperQueryEngine(config)
    
    # è®¾å®šè¯„ä¼°å‚æ•°
    k_value_for_eval = 3
    
    # æ‰§è¡Œè¯„ä¼°
    evaluation_results = evaluate_retrieval_performance(engine, dataset, k_val=k_value_for_eval)
    
    # åˆ†æå¹¶æ‰“å°ç»“æœ
    analyze_and_print_results(evaluation_results, k_val=k_value_for_eval)
    
    # ä¿å­˜ç»“æœ
    save_results(evaluation_results)
    
    logger.info("\n" + "="*80 + "\n=== æµ‹è¯•å®Œæˆ ===\n" + "="*80)

if __name__ == "__main__":
    main()