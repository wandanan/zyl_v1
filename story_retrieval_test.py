# story_retrieval_test.py

import torch
import numpy as np
from typing import List, Tuple, Dict, Any
import time
import json
import logging
import sys

# å¯¼å…¥ä¸»å¼•æ“
# å‡è®¾ advanced_zipper_engine.py åœ¨åŒä¸€ç›®å½•ä¸‹
from advanced_zipper_engine import AdvancedZipperQueryEngine, AdvancedZipperConfig

# --- æ—¥å¿—é…ç½® ---
# é…ç½®æ—¥å¿—ï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('story_retrieval_test.log', mode='w', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


# --- è¾…åŠ©å‡½æ•° ---

def load_story_dataset(json_file_path: str) -> Dict[str, Any]:
    """åŠ è½½æ•…äº‹æ•°æ®é›†"""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        logger.info(f"æˆåŠŸåŠ è½½æ•°æ®é›†: {json_file_path}")
        logger.info(f"åŒ…å« {dataset['metadata']['total_stories']} ä¸ªæ•…äº‹ï¼Œ{dataset['metadata']['total_questions']} ä¸ªé—®é¢˜")
        return dataset
    except FileNotFoundError:
        logger.error(f"é”™è¯¯ï¼šæ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ° -> {json_file_path}")
        logger.error("è¯·ç¡®ä¿ story_qa_dataset.json æ–‡ä»¶ä¸æœ¬è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
        sys.exit(1)
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        raise

def calculate_mrr_at_k(expected: List[int], retrieved: List[int], k: int) -> float:
    """è®¡ç®— Mean Reciprocal Rank @ K"""
    retrieved_k = retrieved[:k]
    for i, doc_id in enumerate(retrieved_k):
        if doc_id in expected:
            return 1.0 / (i + 1)
    return 0.0

def calculate_hit_rate_at_k(expected: List[int], retrieved: List[int], k: int) -> float:
    """è®¡ç®— Hit Rate @ K"""
    return 1.0 if len(set(expected) & set(retrieved[:k])) > 0 else 0.0


# --- æ ¸å¿ƒè¯„ä¼°é€»è¾‘ ---

def evaluate_retrieval_performance(engine: AdvancedZipperQueryEngine, dataset: Dict[str, Any], k: int) -> Dict[str, Any]:
    """è¯„ä¼°æ£€ç´¢æ€§èƒ½"""
    logger.info("=" * 25 + " å¼€å§‹è¯„ä¼°æ£€ç´¢æ€§èƒ½ " + "=" * 25)
    
    all_results = []
    total_questions = 0
    
    # æŒ‰éš¾åº¦å’Œç±»å‹ç»Ÿè®¡
    difficulty_stats = {"easy": [], "medium": [], "hard": []}
    type_stats = {"factual": [], "reasoning": [], "summary": []}
    
    for story in dataset['stories']:
        story_id = story['story_id']
        story_title = story['title']
        story_content = story['content']
        
        logger.info(f"\n--- å¤„ç†æ•…äº‹: {story_title} ({story_id}) ---")
        
        # ã€é‡è¦ã€‘ä¸ºæ¯ä¸ªæ•…äº‹é‡æ–°æ„å»ºç´¢å¼•ï¼Œé¿å…æ•°æ®æ±¡æŸ“
        engine.build_document_index(story_content)
        
        # æµ‹è¯•æ¯ä¸ªé—®é¢˜
        for question_data in story['questions']:
            total_questions += 1
            question_id = question_data['question_id']
            question = question_data['question']
            expected_fragments = question_data['expected_fragments']
            answer = question_data['answer']
            difficulty = question_data['difficulty']
            question_type = question_data['type']
            
            logger.info(f"  -> æµ‹è¯•é—®é¢˜: {question} (ID: {question_id})")
            
            # æ‰§è¡Œæ£€ç´¢
            start_time = time.time()
            results = engine.zipper_retrieve(question, top_k=k)
            retrieval_time = time.time() - start_time
            
            # æå–æ£€ç´¢åˆ°çš„ç‰‡æ®µID
            retrieved_fragments = [doc_id for doc_id, score, doc in results]
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            hit_rate = calculate_hit_rate_at_k(expected_fragments, retrieved_fragments, k)
            mrr = calculate_mrr_at_k(expected_fragments, retrieved_fragments, k)
            
            # è®°å½•ç»“æœ
            result = {
                'story_id': story_id,
                'question': question,
                'expected_fragments': expected_fragments,
                'retrieved_fragments': retrieved_fragments,
                'hit_rate_at_k': hit_rate,
                'mrr_at_k': mrr,
                'retrieval_time': retrieval_time,
                'difficulty': difficulty,
                'type': question_type,
            }
            all_results.append(result)
            
            # æŒ‰éš¾åº¦å’Œç±»å‹åˆ†ç»„ç»Ÿè®¡
            difficulty_stats[difficulty].append(result)
            type_stats[question_type].append(result)
            
            logger.info(f"     æœŸæœ›ç‰‡æ®µ: {expected_fragments}")
            logger.info(f"     æ£€ç´¢ç‰‡æ®µ: {retrieved_fragments}")
            logger.info(f"     Hit@{k}: {'âœ…' if hit_rate > 0 else 'âŒ'}")
            logger.info(f"     MRR@{k}: {mrr:.3f}")
            logger.info(f"     è€—æ—¶: {retrieval_time:.3f}s")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    overall_stats = {
        'total_questions': total_questions,
        'average_mrr_at_k': sum(r['mrr_at_k'] for r in all_results) / len(all_results) if all_results else 0,
        'overall_hit_rate_at_k': sum(r['hit_rate_at_k'] for r in all_results) / len(all_results) if all_results else 0,
        'average_retrieval_time': sum(r['retrieval_time'] for r in all_results) / len(all_results) if all_results else 0,
        'k': k
    }
    
    # åˆ†ç»„ç»Ÿè®¡å‡½æ•°
    def analyze_group(results_group):
        if not results_group: return None
        return {
            'count': len(results_group),
            'average_mrr_at_k': sum(r['mrr_at_k'] for r in results_group) / len(results_group),
            'hit_rate_at_k': sum(r['hit_rate_at_k'] for r in results_group) / len(results_group)
        }

    return {
        'overall_stats': overall_stats,
        'difficulty_analysis': {k: analyze_group(v) for k, v in difficulty_stats.items()},
        'type_analysis': {k: analyze_group(v) for k, v in type_stats.items()},
        'all_results': all_results
    }


# --- ç»“æœå±•ç¤ºä¸ä¿å­˜ ---

def print_evaluation_results(eval_results: Dict[str, Any]):
    """æ‰“å°æ ¼å¼åŒ–çš„è¯„ä¼°ç»“æœ"""
    overall = eval_results['overall_stats']
    k = overall['k']
    
    logger.info("\n" + "="*30 + " æ£€ç´¢æµ‹è¯•ç»“æœæ€»ç»“ " + "="*30)
    
    # æ€»ä½“æƒ…å†µ
    logger.info("\nğŸ“Š æ€»ä½“æ€§èƒ½:")
    logger.info(f"   - æ€»é—®é¢˜æ•°: {overall['total_questions']}")
    logger.info(f"   - å¹³å‡å€’æ•°æ’å (MRR@{k}): {overall['average_mrr_at_k']:.3f}")
    logger.info(f"   - æ€»ä½“å‘½ä¸­ç‡ (HitRate@{k}): {overall['overall_hit_rate_at_k']:.1%}")
    logger.info(f"   - å¹³å‡æ£€ç´¢æ—¶é—´: {overall['average_retrieval_time']:.3f} ç§’/æŸ¥è¯¢")
    
    # æŒ‰é—®é¢˜ç±»å‹åˆ†æ
    logger.info("\nğŸ“ æŒ‰é—®é¢˜ç±»å‹åˆ†æ:")
    for q_type, stats in eval_results['type_analysis'].items():
        if stats:
            logger.info(f"   - {q_type.capitalize()} ({stats['count']}ä¸ª): MRR@{k}={stats['average_mrr_at_k']:.3f}, HitRate@{k}={stats['hit_rate_at_k']:.1%}")
    
    # æŒ‰éš¾åº¦åˆ†æ
    logger.info("\nğŸ“ˆ æŒ‰éš¾åº¦åˆ†æ:")
    for difficulty, stats in eval_results['difficulty_analysis'].items():
        if stats:
            logger.info(f"   - {difficulty.capitalize()} ({stats['count']}ä¸ª): MRR@{k}={stats['average_mrr_at_k']:.3f}, HitRate@{k}={stats['hit_rate_at_k']:.1%}")

    logger.info("="*80)

def save_results(eval_results: Dict[str, Any], config: AdvancedZipperConfig, output_file: str):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°JSONæ–‡ä»¶"""
    logger.info(f"æ­£åœ¨ä¿å­˜è¯¦ç»†æµ‹è¯•ç»“æœåˆ°: {output_file}")
    
    save_data = {
        'model_info': {
            'engine': 'AdvancedZipperQueryEngine',
            'config': config.__dict__ 
        },
        'evaluation_summary': eval_results['overall_stats'],
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'details': {
            'difficulty_analysis': eval_results['difficulty_analysis'],
            'type_analysis': eval_results['type_analysis'],
            'individual_results': eval_results['all_results']
        }
    }
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ç»“æœå·²æˆåŠŸä¿å­˜ã€‚")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")


# --- ä¸»å‡½æ•° ---

def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•´ä¸ªæµ‹è¯•æµç¨‹"""
    logger.info("="*80)
    logger.info("=== çŠ¶æ€åŒ–æµå¼æ‹‰é“¾æ£€ç´¢å™¨ - æ•…äº‹QAæµ‹è¯•è„šæœ¬ ===")
    
    # --- ç³»ç»Ÿä¸ç¯å¢ƒæ£€æŸ¥ ---
    logger.info("\n--- ç¯å¢ƒæ£€æŸ¥ ---")
    logger.info(f"Python ç‰ˆæœ¬: {sys.version.split()[0]}")
    logger.info(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # --- é…ç½®ä¸åˆå§‹åŒ– ---
    logger.info("\n--- åˆå§‹åŒ–å¼•æ“ ---")
    
    # ã€é‡è¦ã€‘ä½¿ç”¨ä¸æ–°å¼•æ“åŒ¹é…çš„é…ç½®
    config = AdvancedZipperConfig(
        num_heads=8,
        sparse_top_n=50,  # BM25å¬å›50ä¸ªå€™é€‰ï¼Œå¯ä»¥è°ƒæ•´
        context_memory_decay=0.8, # è®°å¿†è¡°å‡ç‡
        use_bge_embedding=True
    )
    # top_k å°†åœ¨è¯„ä¼°å‡½æ•°ä¸­æŒ‡å®šï¼Œè¿™é‡Œæ— éœ€é…ç½®
    
    engine = AdvancedZipperQueryEngine(config)
    
    # --- æ•°æ®åŠ è½½ ---
    logger.info("\n--- åŠ è½½æ•°æ®é›† ---")
    dataset = load_story_dataset('story_qa_dataset.json')
    
    # --- æ€§èƒ½è¯„ä¼° ---
    k_for_evaluation = 3 # æˆ‘ä»¬å…³å¿ƒTop-3çš„ç»“æœ
    evaluation_results = evaluate_retrieval_performance(engine, dataset, k=k_for_evaluation)
    
    # --- ç»“æœå±•ç¤ºä¸ä¿å­˜ ---
    print_evaluation_results(evaluation_results)
    save_results(evaluation_results, config, "retrieval_results.json")
    
    logger.info("\n" + "="*80)
    logger.info("=== æµ‹è¯•å®Œæˆ ===")
    logger.info("="*80)

if __name__ == "__main__":
    main()