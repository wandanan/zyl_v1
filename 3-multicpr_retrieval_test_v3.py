# multicpr_retrieval_test_v3.py

import torch
import numpy as np
from typing import List, Tuple, Dict, Any
import time
import json
import logging
import sys
import os
import pandas as pd
from collections import defaultdict

# å¯¼å…¥V3å¼•æ“ï¼
from advanced_zipper_engine_v3 import AdvancedZipperQueryEngineV3, ZipperV3Config, ZipperV3State

# --- é…ç½®å‚æ•° ---
MAX_DOCS = 5000   # é™åˆ¶æ€»æ–‡æ¡£æ•°ï¼ŒåŠ å¿«æµ‹è¯•
DOMAIN = 'video'   # æµ‹è¯•åŸŸ: 'ecom', 'medical', 'video'
MAX_QUERIES = 100 # å¯ä»¥é€‚å½“å¢åŠ æµ‹è¯•æŸ¥è¯¢æ•°ï¼Œä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
K_VALUES = [1, 3, 5, 10]  # è¯„ä¼°çš„Kå€¼

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('multicpr_retrieval_test_v3.log', mode='w', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# --- æ•°æ®åŠ è½½å‡½æ•° ---
def load_multicpr_data(data_dir: str, domain: str = 'ecom', max_docs: int = None) -> Dict[str, Any]:
    logger.info(f"æ­£åœ¨åŠ è½½Multi-CPR {domain}åŸŸæ•°æ®...")
    corpus_file = os.path.join(data_dir, domain, 'corpus.tsv')
    dev_query_file = os.path.join(data_dir, domain, 'dev.query.txt')
    qrels_file = os.path.join(data_dir, domain, 'qrels.dev.tsv')
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for file_path in [corpus_file, dev_query_file, qrels_file]:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # é¢„åŠ è½½æ‰€æœ‰ç›¸å…³PID
    required_pids = set()
    df_qrels_full = pd.read_csv(qrels_file, sep='\t', header=None, names=['qid', 'iter', 'pid', 'relevance'])
    qrels_pids = set(df_qrels_full[df_qrels_full['relevance'] == 1]['pid'].unique())
    logger.info(f"éœ€è¦ä¼˜å…ˆåŠ è½½çš„ç›¸å…³PIDæ•°é‡: {len(qrels_pids)}")
    
    # ä½¿ç”¨æ›´ç¨³å¥çš„é€è¡Œè§£ææ–¹å¼è¯»å– TSVï¼ˆä»…æŒ‰ç¬¬ä¸€ä¸ªåˆ¶è¡¨ç¬¦åˆ†å‰²ï¼Œå®¹å¿å†…å®¹ä¸­çš„å¼•å·/åˆ¶è¡¨ç¬¦ï¼‰
    malformed_corpus_lines = 0
    total_corpus_lines = 0
    required_rows: List[Tuple[int, str]] = []
    other_rows: List[Tuple[int, str]] = []

    def parse_two_column_tsv(filepath: str):
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                yield line.rstrip('\n')

    for line in parse_two_column_tsv(corpus_file):
        total_corpus_lines += 1
        if not line:
            continue
        if '\t' not in line:
            malformed_corpus_lines += 1
            continue
        pid_str, content = line.split('\t', 1)
        try:
            pid = int(pid_str)
        except ValueError:
            malformed_corpus_lines += 1
            continue
        if pid in qrels_pids:
            required_rows.append((pid, content))
        else:
            other_rows.append((pid, content))

    if max_docs and len(required_rows) < max_docs:
        needed = max_docs - len(required_rows)
        df_final_rows = required_rows + other_rows[:max(0, needed)]
    elif max_docs:
        df_final_rows = required_rows[:max_docs]
    else:
        df_final_rows = required_rows + other_rows

    corpus = {pid: content for pid, content in df_final_rows}

    # è¯»å–æŸ¥è¯¢ï¼ŒåŒæ ·é‡‡ç”¨ç¨³å¥è§£æ
    malformed_query_lines = 0
    total_query_lines = 0
    queries: Dict[int, str] = {}
    for line in parse_two_column_tsv(dev_query_file):
        total_query_lines += 1
        if not line or '\t' not in line:
            malformed_query_lines += 1
            continue
        qid_str, query_text = line.split('\t', 1)
        try:
            qid = int(qid_str)
        except ValueError:
            malformed_query_lines += 1
            continue
        queries[qid] = query_text
    
    # æ„å»º qrels å­—å…¸
    qrels = defaultdict(list)
    for _, row in df_qrels_full[df_qrels_full['relevance'] == 1].iterrows():
        qrels[row['qid']].append(row['pid'])
    
    logger.info(f"æ•°æ®åŠ è½½å®Œæˆï¼Œè¯­æ–™åº“: {len(corpus)}ï¼ˆåŸå§‹è¡Œæ•°: {total_corpus_lines}ï¼Œå¼‚å¸¸è¡Œ: {malformed_corpus_lines}ï¼‰ï¼ŒæŸ¥è¯¢: {len(queries)}ï¼ˆåŸå§‹è¡Œæ•°: {total_query_lines}ï¼Œå¼‚å¸¸è¡Œ: {malformed_query_lines}ï¼‰ï¼Œæ ‡æ³¨: {len(qrels)}")
    return {'corpus': corpus, 'queries': queries, 'qrels': dict(qrels), 'domain': domain}

# --- è¯„ä¼°æŒ‡æ ‡è®¡ç®— ---
def calculate_metrics(expected: List[int], retrieved: List[int], k_values: List[int]) -> Dict[str, float]:
    metrics = {}
    for k in k_values:
        retrieved_k = retrieved[:k]
        hit = len(set(expected) & set(retrieved_k)) > 0
        metrics[f'hit_rate_at_{k}'] = 1.0 if hit else 0.0
        metrics[f'recall_at_{k}'] = len(set(expected) & set(retrieved_k)) / len(expected) if expected else 0.0
        
        rr = 0.0
        for i, doc_id in enumerate(retrieved_k):
            if doc_id in expected:
                rr = 1.0 / (i + 1)
                break
        metrics[f'mrr_at_{k}'] = rr
    return metrics

# --- V3 æ ¸å¿ƒè¯„ä¼°é€»è¾‘ ---
def evaluate_retrieval_performance_v3(engine: AdvancedZipperQueryEngineV3, dataset: Dict[str, Any]):
    config = engine.config
    logger.info("=" * 25 + f" å¼€å§‹è¯„ä¼°V3å¼•æ“åœ¨Multi-CPRä¸Šçš„æ€§èƒ½ " + "=" * 25)
    
    engine.build_document_index(dataset['corpus'])
    
    all_results = []
    # ç­›é€‰å‡ºæœ‰æ ‡æ³¨çš„æŸ¥è¯¢è¿›è¡Œæµ‹è¯•
    queries_to_test = {k: v for k, v in dataset['queries'].items() if k in dataset['qrels']}
    
    # æ¨¡æ‹Ÿä¼šè¯çŠ¶æ€
    session_state = ZipperV3State(original_query="", context_vector=torch.zeros(config.embedding_dim, device=device))

    test_count = 0
    for qid, query in queries_to_test.items():
        if test_count >= MAX_QUERIES:
            logger.info(f"å·²è¾¾åˆ°æœ€å¤§æŸ¥è¯¢æ•°é™åˆ¶ ({MAX_QUERIES})ï¼Œåœæ­¢æµ‹è¯•ã€‚")
            break
        
        expected_pids = dataset['qrels'][qid]
        logger.info(f"--- ({test_count+1}/{MAX_QUERIES}) QID:{qid} -> {query[:50]}...")
        
        results = engine.retrieve(query, state=session_state)
        session_state = engine.update_state(session_state, results) # æ›´æ–°çŠ¶æ€
        
        retrieved_pids = [pid for pid, score, content in results]
        
        result_entry = {
            'qid': qid,
            'query': query,
            'expected_pids': expected_pids,
            'retrieved_pids': retrieved_pids,
            'metrics': calculate_metrics(expected_pids, retrieved_pids, K_VALUES)
        }
        all_results.append(result_entry)
        test_count += 1
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    overall_stats = {'total_queries': len(all_results), 'metrics': {}}
    if not all_results:
        logger.warning("æ²¡æœ‰å¯è¯„ä¼°çš„ç»“æœã€‚")
        return overall_stats
        
    for k in K_VALUES:
        for metric_name in [f'mrr_at_{k}', f'hit_rate_at_{k}', f'recall_at_{k}']:
            avg_metric_name = f'average_{metric_name}'
            overall_stats['metrics'][avg_metric_name] = np.mean([r['metrics'][metric_name] for r in all_results])
            
    print_evaluation_results(overall_stats, config)
    return overall_stats

# --- ç»“æœå±•ç¤º ---
def print_evaluation_results(overall_stats: Dict[str, Any], config: ZipperV3Config):
    if not overall_stats.get('total_queries'): return
    logger.info("\n" + "="*30 + " V3å¼•æ“ Multi-CPR æµ‹è¯•ç»“æœ " + "="*30)
    logger.info(f"\nğŸ“Š æ€»ä½“æ€§èƒ½ (æŸ¥è¯¢æ•°: {overall_stats['total_queries']}):")
    logger.info(f"   - ç­–ç•¥: {'Hybrid' if config.use_hybrid_search else 'ColBERT only'}, {'Multi-Head' if config.use_multi_head else 'Single-Head'}, {'Stateful' if config.use_stateful_reranking else 'Stateless'}")
    
    logger.info("\nğŸ“ˆ å„Kå€¼å¹³å‡æ€§èƒ½:")
    header = f"   {'K':<5}{'MRR':<10}{'Hit Rate':<12}{'Recall':<10}"
    logger.info(header + "\n" + "   " + "-"*len(header))
    for k in K_VALUES:
        mrr = overall_stats['metrics'].get(f'average_mrr_at_{k}', 0)
        hit_rate = overall_stats['metrics'].get(f'average_hit_rate_at_{k}', 0)
        recall = overall_stats['metrics'].get(f'average_recall_at_{k}', 0)
        logger.info(f"   {k:<5}{mrr:<10.3f}{hit_rate:<12.1%}{recall:<10.1%}")
    logger.info("="*80)

# --- ä¸»å‡½æ•° ---
def main():
    logger.info("="*80)
    logger.info("=== V3å¼•æ“åœ¨Multi-CPRä¸Šçš„æœ€ç»ˆæ€§èƒ½è¯„ä¼°è„šæœ¬ ===")
    
    # --- åœ¨è¿™é‡Œé…ç½®ä½ çš„å®éªŒç­–ç•¥ ---
    # è¿™æ˜¯ä¸€å¥—æ¨èçš„ã€åœ¨ä¹‹å‰æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²çš„â€œå…¨åŠŸèƒ½â€é…ç½®
    config = ZipperV3Config(
        bge_model_path="models--BAAI--bge-large-zh-v1.5/snapshots/79e7739b6ab944e86d6171e44d24c997fc1e0116", # ç¡®ä¿ä½¿ç”¨å¼ºå¤§çš„largeæ¨¡å‹
        embedding_dim=1024,
        use_hybrid_search=True,
        bm25_weight=1.0,
        colbert_weight=1.5,      # ç»™äºˆè¯­ä¹‰æ¨¡å‹æ›´é«˜çš„æƒé‡
        use_multi_head=True,
        use_length_penalty=True,
        use_stateful_reranking=True, # å°½ç®¡åœ¨å•è½®è¯„ä¼°ä¸­ä½œç”¨æœ‰é™ï¼Œä½†æˆ‘ä»¬ä¿æŒæ¶æ„å®Œæ•´æ€§
        context_influence=0.3,
        # --- å…¨é‡é¢„ç¼–ç ä¸AMPè‡ªé€‚åº” ---
        precompute_doc_tokens=False,           # False=æŒ‰éœ€ç¼–ç , True=å…¨é‡é¢„ç¼–ç 
        enable_amp_if_beneficial=True          # è‡ªåŠ¨æ ¹æ®æ˜¾å¡é€‰æ‹©æ˜¯å¦å¯ç”¨ AMP åŠå…¶ç²¾åº¦
    )
    
    engine = AdvancedZipperQueryEngineV3(config)
    
    data_dir = "Multi-CPR/data"
    dataset = load_multicpr_data(data_dir, DOMAIN, max_docs=MAX_DOCS)

    evaluate_retrieval_performance_v3(engine, dataset)

if __name__ == "__main__":
    main()